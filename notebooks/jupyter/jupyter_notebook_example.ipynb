{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKIL Clients\n",
    "This notebook tutorial demonstrate how to leverage SKIL from an external system (a raw Jupyter Notebook, in this case) using [skil-clients](https://github.com/SkymindIO/skil-clients).\n",
    "\n",
    "## Starting a Jupyter Server\n",
    "You can start a Jupyter server in either of the two following ways:\n",
    "## [Method - 1]  Starting a Jupyter Server in SKIL \n",
    "\n",
    "SKIL v1.1 and later comes pre-bundled with a jupyter notebook package. You can start a Jupyter server by running the following script in a Zeppelin notebook's paragraph.\n",
    "```\n",
    "%sh\n",
    "nohup /opt/skil/miniconda/bin/jupyter notebook --ip=0.0.0.0 --port=8888 --notebook-dir='~' &\n",
    "```\n",
    "\n",
    "This will give you the link to connect to the Jupyter server. If you get a connection string similar to the following:\n",
    "```\n",
    "http://ffasdawfga:8888/?token=135a1ced8a4fd35790d932f0f23fbb0ad8484374f3aa97a2\n",
    "```\n",
    "then make sure you change the **host** part to either `0.0.0.0` or `localhost` or the **domain name/ip address of your server**. So, the connection string would look something like this:\n",
    "```\n",
    "http://0.0.0.0:8888/?token=135a1ced8a4fd35790d932f0f23fbb0ad8484374f3aa97a2\n",
    "OR\n",
    "http://localhost:8888/?token=135a1ced8a4fd35790d932f0f23fbb0ad8484374f3aa97a2\n",
    "OR\n",
    "http://<your_domain_name>:8888/?token=135a1ced8a4fd35790d932f0f23fbb0ad8484374f3aa97a2\n",
    "```\n",
    "\n",
    "Also, if you're running SKIL through the [docker image](https://hub.docker.com/r/skymindops/skil-ce/), make sure you add additional port forwards using `-p 8888:8888` and `-p 9100:9100` to the `docker run` command to access the Jupyter instance in your browser and the model history server endpoints, respectively. So, your docker command could look something like this: \n",
    "```\n",
    "docker run --rm -it -p 9008:9008 -p 8080:8080 -p 9100:9100 -p 8888:8888 skymindops/skil-ce bash /start-skil.sh\n",
    "```\n",
    "\n",
    "## [Method - 2] Starting a Jupyter Server externally\n",
    "If you want to install Jupyter in another machine then you can follow this guide [here](http://jupyter.org/install).\n",
    "\n",
    "After you've started the Jupyter server successfully, you can import/upload this notebook to it and start working on it.\n",
    "\n",
    "## Goals\n",
    "This tutorial will target the following features of SKIL:\n",
    "1. Upload a trained model to the SKIL server\n",
    "2. Utilize the [skil-clients](https://github.com/SkymindIO/skil-clients) API ([Python](https://github.com/SkymindIO/skil-clients/tree/master/python) version) \n",
    "3. Utilize the \"Model History Server\" API (from skil-clients) to:\n",
    "    - Create workspaces,\n",
    "    - Create experiments\n",
    "    - Adding models\n",
    "    - Adding evaluations\n",
    "    - Adding minibatches\n",
    "    - Adding examples\n",
    "    - Calculating evaluations through the model feedback endpoint\n",
    "4. Utilize the \"Deployments\" API (from skil-clients) to:\n",
    "    - Deploy a model\n",
    "    - Start a model to serve.\n",
    "5. Utilize the \"Predictions\" API (from skil-clients) to:\n",
    "    - Classify an image\n",
    "    \n",
    "We'll install TensorFlow to train a basic MNIST model. Then we'll upload that model to the SKIL server. Later on we'll add the model into an experiment in a workspace and then add evaluations to it based on the training and test results obtain through the TensorFlow model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing TensorFlow\n",
    "You can skip this step if you're using SKIL or if you already have TensorFlow installed and linked to your Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sx pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and installing the skil-clients python API\n",
    "The [skil-clients](https://github.com/SkymindIO/skil-clients) API provides an easy way to utilize the SKIL's REST API in different languages. Here, the python version is demonstrated. Let's first install the skil-clients python API. You can skip this step too if you already have the \"skil-clients\" API installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sx pip install skil-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "You might have to restart the kernel if the endpoints API is not properly loaded. To do that, go to `Kernel` in the top menu bar and then click `Restart`. It's recommended that you follow this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "This is a basic MNIST model for demonstrating how SKIL can work with a trained TensorFlow model. You can find more examples of different TensorFlow models on their example repo [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def deepnn(x):\n",
    "  \"\"\"deepnn builds the graph for a deep net for classifying digits.\n",
    "  Args:\n",
    "    x: an input tensor with the dimensions (N_examples, 784), where 784 is the\n",
    "    number of pixels in a standard MNIST image.\n",
    "  Returns:\n",
    "    A tuple (y, keep_prob). y is a tensor of shape (N_examples, 10), with values\n",
    "    equal to the logits of classifying the digit into one of 10 classes (the\n",
    "    digits 0-9). keep_prob is a scalar placeholder for the probability of\n",
    "    dropout.\n",
    "  \"\"\"\n",
    "  # Reshape to use within a convolutional neural net.\n",
    "  # Last dimension is for \"features\" - there is only one here, since images are\n",
    "  # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n",
    "  x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "  # First convolutional layer - maps one grayscale image to 32 feature maps.\n",
    "  W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "  b_conv1 = bias_variable([32])\n",
    "  h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "  # Pooling layer - downsamples by 2X.\n",
    "  h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "  # Second convolutional layer -- maps 32 feature maps to 64.\n",
    "  W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "  b_conv2 = bias_variable([64])\n",
    "  h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "  # Second pooling layer.\n",
    "  h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "  # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n",
    "  # is down to 7x7x64 feature maps -- maps this to 1024 features.\n",
    "  W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "  b_fc1 = bias_variable([1024])\n",
    "\n",
    "  h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "  h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "  # Dropout - controls the complexity of the model, prevents co-adaptation of\n",
    "  # features.\n",
    "  keep_prob = tf.placeholder(tf.float32, name=\"keep_prob_input\")\n",
    "  h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "  # Map the 1024 features to 10 classes, one for each digit\n",
    "  W_fc2 = weight_variable([1024, 10])\n",
    "  b_fc2 = bias_variable([10])\n",
    "\n",
    "  y_conv = tf.add(tf.matmul(h_fc1_drop, W_fc2), b_fc2) \n",
    "  return y_conv, keep_prob\n",
    " \n",
    "\n",
    "def conv2d(x, W):\n",
    "  \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "  \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "  \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Freezing the Tensorflow Model\n",
    "After training the TensorFlow model, you'll have to freeze it into a `.pb` file in order to be able to upload it to the SKIL server. By freezing the TensorFlow model, you're actually saving the weights and graph details into a single file which the SKIL server can understand and deploy for serving. You can look at the TensorFlow documentation for freezing a model [here](https://www.tensorflow.org/extend/tool_developers/#freezing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-c4c6e4e250d1>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\shams\\miniconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\shams\\miniconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From c:\\users\\shams\\miniconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data_directory\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From c:\\users\\shams\\miniconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data_directory\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\shams\\miniconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data_directory\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data_directory\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\shams\\miniconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-2-c4c6e4e250d1>:23: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "\n",
      "Training model...\n",
      "step 0, training accuracy 0.04\n",
      "step 100, training accuracy 0.78\n",
      "step 200, training accuracy 0.78\n",
      "step 300, training accuracy 0.8\n",
      "step 400, training accuracy 0.9\n",
      "step 500, training accuracy 0.94\n",
      "step 600, training accuracy 0.96\n",
      "step 700, training accuracy 0.96\n",
      "step 800, training accuracy 0.94\n",
      "step 900, training accuracy 0.92\n",
      "step 1000, training accuracy 0.98\n",
      "\n",
      "Testing model...\n",
      "test accuracy 0.9655\n",
      "\n",
      "Saving checkpoint...\n",
      "\n",
      "Freezing graph...\n",
      "INFO:tensorflow:Restoring parameters from data_directory\\saved_checkpoint-0\n",
      "INFO:tensorflow:Froze 8 variables.\n",
      "INFO:tensorflow:Converted 8 variables to const ops.\n",
      "\n",
      "Graph frozen successfully!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.training import saver as saver_lib\n",
    "from tensorflow.python.framework import graph_io\n",
    "\n",
    "# Import data\n",
    "work_directory = 'data_directory'\n",
    "saver_write_version = 2\n",
    "\n",
    "mnist = input_data.read_data_sets(work_directory, one_hot=True)\n",
    "\n",
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 784], name=\"input_node\")\n",
    "\n",
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder(tf.float32, [None, 10], name=\"input_labels_node\")\n",
    "\n",
    "# Build the graph for the deep net\n",
    "y_conv, keep_prob = deepnn(x)\n",
    "\n",
    "softmax = tf.nn.softmax(y_conv, name=\"output_node\")\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "checkpoint_prefix = os.path.join(work_directory, \"saved_checkpoint\")\n",
    "checkpoint_meta_graph_file = os.path.join(work_directory,\n",
    "                                          \"saved_checkpoint.meta\")\n",
    "checkpoint_state_name = \"checkpoint_state\"\n",
    "input_graph_name = \"input_graph.pb\"\n",
    "output_graph_name = \"output_graph.pb\"\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "\n",
    "# We'll later add these model accuraries to the uploaded SKIL model evaluation, manually.\n",
    "train_accuracy = 0\n",
    "test_accuracy = 0\n",
    "\n",
    "# Later, we will use the two arrays below to calculate the model evaluation through the feedback endpoint\n",
    "test_guesses = [] # This will contain the predicted labels array\n",
    "test_correct = [] # This will contain the actual labels array\n",
    "\n",
    "steps = 1001\n",
    "test_images = mnist.test.images\n",
    "test_labels = mnist.test.labels\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  for i in range(steps):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i % 100 == 0:\n",
    "      train_accuracy = accuracy.eval(feed_dict={\n",
    "          x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "      print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "  print('\\nTesting model...')    \n",
    "  # These two string arrays will be used for the feedback endpoint at the end of the notebook\n",
    "  test_guesses = tf.argmax(y_conv, 1).eval(feed_dict={x: test_images, keep_prob: 1.0}).astype(str)\n",
    "  test_correct = tf.argmax(y_, 1).eval(feed_dict={y_: test_labels}).astype(str)\n",
    "  \n",
    "  test_accuracy = accuracy.eval(feed_dict={\n",
    "      x: test_images, y_: test_labels, keep_prob: 1.0})\n",
    "  print('test accuracy %g' % test_accuracy)\n",
    "  \n",
    "  print(\"\\nSaving checkpoint...\")\n",
    "\n",
    "  saver = saver_lib.Saver(write_version=saver_write_version)\n",
    "  checkpoint_path = saver.save(\n",
    "      sess,\n",
    "      checkpoint_prefix,\n",
    "      global_step=0,\n",
    "      latest_filename=checkpoint_state_name)\n",
    "  graph_io.write_graph(sess.graph, work_directory, input_graph_name)\n",
    "\n",
    "  input_graph_path = os.path.join(work_directory, input_graph_name)\n",
    "  input_saver_def_path = \"\"\n",
    "  input_binary = False\n",
    "  output_node_names = \"output_node\"\n",
    "  restore_op_name = \"save/restore_all\"\n",
    "  filename_tensor_name = \"save/Const:0\"\n",
    "  output_graph_path = os.path.join(work_directory, output_graph_name)\n",
    "  clear_devices = False\n",
    "  input_meta_graph = checkpoint_meta_graph_file\n",
    "\n",
    "  print(\"\\nFreezing graph...\")\n",
    "    \n",
    "  freeze_graph.freeze_graph(\n",
    "        input_graph_path,\n",
    "        input_saver_def_path,\n",
    "        input_binary,\n",
    "        checkpoint_path,\n",
    "        output_node_names,\n",
    "        restore_op_name,\n",
    "        filename_tensor_name,\n",
    "        output_graph_path,\n",
    "        clear_devices,\n",
    "        \"\",\n",
    "        \"\",\n",
    "        input_meta_graph,\n",
    "        checkpoint_version=saver_write_version)\n",
    "  print(\"\\nGraph frozen successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Authenticating the clients \n",
    "Let's create the necessary API instances for utilizing the REST services. \n",
    "\n",
    "### Note\n",
    "Since there can be multiple model history servers, you will have to know its process GUID in order to interact with it through \"skil-clients\". You can find the SKIL Process details with the `skil services` command as:\n",
    "```\n",
    "$SKIL_HOME/sbin/skil login --userId admin --password admin # You might have a different userId and password, replace them accordingly. SKIL_HOME is normally '/opt/skil' \n",
    "\n",
    "$SKIL_HOME/sbin/skil services \n",
    "```\n",
    "\n",
    "This will give you a list of every process with its details. For filtering the CLI output, you'd need the [JQ](https://stedolan.github.io/jq/) tool. You can download it [here](https://stedolan.github.io/jq/download/).\n",
    "\n",
    "#### For CentOS, the JQ installation is as follows:\n",
    "```\n",
    "wget -O jq https://github.com/stedolan/jq/releases/download/jq-1.5/jq-linux64\n",
    "sudo chmod +x ./jq\n",
    "sudo cp jq /usr/bin\n",
    "```\n",
    "\n",
    "### Finding out the model history server ID:\n",
    "After installing JQ, you can find out the model history server details as follows:\n",
    "```\n",
    "$SKIL_HOME/sbin/skil services >> file.json && tail -c +7 file.json | jq '.serviceInfoList[] | select(.type == \"modelhistory\") | {name: .name, type: .type, guid: .id}'\n",
    "```\n",
    "\n",
    "After that, you can add the value of the 'guid' key for the desired model history server from the command output to the following paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add the Model History Server ID here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history_server_id = \"<insert_model_history_server_id>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticating with SKIL API...\n",
      "{'token': 'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJhdWQiOiJTa2lsVXNlciIsInN1YiI6IntcInVzZXJJZFwiOlwiYWRtaW5cIixcInVzZXJOYW1lXCI6XCJhZG1pblwiLFwicGFzc3dvcmRcIjpcImFkbWluXCIsXCJyb2xlXCI6XCJhZG1pblwiLFwic2NvcGVcIjpcImFkbWluXCJ9IiwiaXNzIjoiU2tpbEF1dGhNYW5hZ2VyIiwiZXhwIjoxNTM2NTkxOTkzLCJpYXQiOjE1MzY1MDU1OTN9.rIX6AazqMrr1ZFj9n1hX5h6ixRx7fSGoPvJrNDMWuSw'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import unittest\n",
    "\n",
    "import numpy\n",
    "import skil_client\n",
    "from skil_client import *\n",
    "from skil_client.rest import ApiException\n",
    "\n",
    "debug = False\n",
    "\n",
    "host = \"localhost\" # Rename this to the host you are using \n",
    "\n",
    "config = Configuration()\n",
    "config.host = \"{}:9008\".format(host)  # change this if you're using a different port number for the general API!\n",
    "config.debug = debug\n",
    "api_client = ApiClient(configuration=config)\n",
    "# create an instance of the API class\n",
    "api_instance = skil_client.DefaultApi(api_client=api_client)\n",
    "\n",
    "# authenticate\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "try:\n",
    "    print(\"Authenticating with SKIL API...\")\n",
    "    credentials = skil_client.Credentials(user_id=\"admin\", password=\"admin\") # Update this with the ID and password you're using for your SKIL server\n",
    "    token = api_instance.login(credentials)\n",
    "    pp.pprint(token)\n",
    "    # add credentials to config\n",
    "    config.api_key['authorization'] = token.token\n",
    "    config.api_key_prefix['authorization'] = \"Bearer\"\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DefaultApi->login: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading the Frozen Model file\n",
    "To start working with the frozen model, it needs to be present on the SKIL server. You can upload a model file in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading model, please wait...\n",
      "{'file_upload_response_list': [{'file_content': None,\n",
      "                                'file_name': 'output_graph.pb',\n",
      "                                'key': 'file',\n",
      "                                'path': '/opt/skil/plugins/files/MODEL/output_graph.pb',\n",
      "                                'status': 'uploaded',\n",
      "                                'type': None}]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Uploading model, please wait...\")\n",
    "modelFile = os.path.join(work_directory, output_graph_name)\n",
    "uploads = api_instance.upload(file=modelFile)\n",
    "pp.pprint(uploads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the model file path\n",
    "This will give the path of the model file uploaded and stored on the server. This will be without the file schema (`file://` or `hdfs://`). So, this will have to be added manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'file:///opt/skil/plugins/files/MODEL/output_graph.pb'\n"
     ]
    }
   ],
   "source": [
    "model_file_path = \"file://\" + uploads.file_upload_response_list[0].path\n",
    "pp.pprint(model_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Workspace/Model History\n",
    "Workspace and Model History means the same thing in SKIL's context. They are used to store the experiments for the models and their particular details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created': 1536506956524,\n",
      " 'model_history_id': 'dec0bbde-bf81-45cf-b223-f88c24d0ff99',\n",
      " 'model_labels': 'Jupyter, python, tensorflow',\n",
      " 'model_name': 'jupyter_workspace'}\n"
     ]
    }
   ],
   "source": [
    "add_model_history_response = api_instance.add_model_history(\n",
    "    model_history_server_id,\n",
    "    AddModelHistoryRequest(\n",
    "        \"jupyter_workspace\",\n",
    "        \"Jupyter, python, tensorflow\"\n",
    "    )\n",
    ")\n",
    "\n",
    "pp.pprint(add_model_history_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Here the `model_name` and `model_labels` actually refers to the model history (aka workspace) name and the model history labels and shouldn't be confused with a model instance name and its labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding an Experiment to the Workspace\n",
    "You can add an experiment to the workspace without having to add the details of a Zeppelin Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_model_id': None,\n",
      " 'experiment_description': 'Leveraging SKIL from a Jupyter notebook',\n",
      " 'experiment_id': 'jupyter_experiment_12345',\n",
      " 'experiment_name': 'jupyter_experiment',\n",
      " 'input_data_uri': None,\n",
      " 'last_updated': None,\n",
      " 'model_history_id': 'dec0bbde-bf81-45cf-b223-f88c24d0ff99',\n",
      " 'notebook_json': None,\n",
      " 'notebook_url': None,\n",
      " 'zeppelin_id': None}\n"
     ]
    }
   ],
   "source": [
    "model_history_id = add_model_history_response.model_history_id\n",
    "\n",
    "experiment_id = \"jupyter_experiment_12345\"\n",
    "\n",
    "add_experiment_response = api_instance.add_experiment(\n",
    "    model_history_server_id,\n",
    "    ExperimentEntity(\n",
    "        experiment_id=experiment_id,\n",
    "        experiment_name=\"jupyter_experiment\",\n",
    "        experiment_description=\"Leveraging SKIL from a Jupyter notebook\",\n",
    "        model_history_id=model_history_id\n",
    "    )\n",
    ")\n",
    "\n",
    "pp.pprint(add_experiment_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a model to an experiment\n",
    "Now that the workspace is created, we can add and register our uploaded model to the experiment inside it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created': 1536506955773,\n",
      " 'etl_json': None,\n",
      " 'eval_id': None,\n",
      " 'experiment_id': 'jupyter_experiment_12345',\n",
      " 'input_formats': None,\n",
      " 'model_id': 'jupyter_mnist_model_12345',\n",
      " 'model_labels': '0, 1, 2, 3, 4, 5, 6, 7, 8, 9',\n",
      " 'model_name': 'Jupyter MNIST',\n",
      " 'model_version': '1',\n",
      " 'notebook_json': None,\n",
      " 'original_model_id': None,\n",
      " 'uri': 'file:///opt/skil/plugins/files/MODEL/output_graph.pb'}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model_id = \"jupyter_mnist_model_12345\"\n",
    "\n",
    "add_model_instance_response = api_instance.add_model_instance(\n",
    "    model_history_server_id,\n",
    "    ModelInstanceEntity(\n",
    "        uri=model_file_path,\n",
    "        model_id=model_id,\n",
    "        model_labels=\"0, 1, 2, 3, 4, 5, 6, 7, 8, 9\", # Comma-separated MNIST labels (The format very important for the feedback endpoint)\n",
    "        model_name=\"Jupyter MNIST\",\n",
    "        model_version=\"1\",\n",
    "        created=int(round(time.time() * 1000)),\n",
    "        experiment_id=experiment_id\n",
    "    )\n",
    ")\n",
    "\n",
    "pp.pprint(add_model_instance_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Pay special attention to the `model_labels` argument. This corresponds to the comma-separated label names of the model. For example, for CIFAR-10 dataset this would be: `airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck`. The order of the labels should be exactly the same as the order you have it in your labels file and how you're encoding your one-hot vectors for training and evaluation. This will be used later when we'll automatically calculate the evaluation scores based on an array of predicted and actual labels through the \"feedback\" endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding evaluations to a model\n",
    "Let's add the accuracy of the model as an evaluation to the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.965499997138977,\n",
      " 'auc': 0.0,\n",
      " 'binary_threshold': 0.0,\n",
      " 'binary_thresholds': None,\n",
      " 'created': 1536507016258,\n",
      " 'eval_id': 'jupyter_model_eval_id_test',\n",
      " 'eval_name': 'MNIST Test Accuracy',\n",
      " 'eval_version': 2,\n",
      " 'evaluation': '',\n",
      " 'f1': 0.0,\n",
      " 'mean_absolute_error': 0.0,\n",
      " 'mean_relative_error': 0.0,\n",
      " 'model_instance_id': 'jupyter_mnist_model_12345',\n",
      " 'precision': 0.0,\n",
      " 'r2': 0.0,\n",
      " 'recall': 0.0,\n",
      " 'rmse': 0.0}\n"
     ]
    }
   ],
   "source": [
    "eval_id_train = \"jupyter_model_eval_id_train\"\n",
    "eval_id_test = \"jupyter_model_eval_id_test\"\n",
    "\n",
    "eval_created_time = int(round(time.time() * 1000))\n",
    "\n",
    "eval_response_train = api_instance.add_evaluation_result(\n",
    "    model_history_server_id,\n",
    "    EvaluationResultsEntity(\n",
    "        evaluation=\"\",\n",
    "        created=eval_created_time,\n",
    "        eval_name=\"MNIST Train Accuracy\",\n",
    "        model_instance_id=model_id,\n",
    "        accuracy=float(train_accuracy),\n",
    "        eval_id=eval_id_train,\n",
    "        eval_version=1\n",
    "    )\n",
    ")\n",
    "\n",
    "pp.pprint(eval_response_train)\n",
    "\n",
    "eval_response_test = api_instance.add_evaluation_result(\n",
    "    model_history_server_id,\n",
    "    EvaluationResultsEntity(\n",
    "        evaluation=\"\",\n",
    "        created=eval_created_time,\n",
    "        eval_name=\"MNIST Test Accuracy\",\n",
    "        model_instance_id=model_id,\n",
    "        accuracy=float(test_accuracy),\n",
    "        eval_id=eval_id_test,\n",
    "        eval_version=2\n",
    "    )\n",
    ")\n",
    "\n",
    "pp.pprint(eval_response_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating evaluation through Feedback endpoint\n",
    "Currently, there is a pattern of steps you have to use to take advantage of the Feedback endpoint. This design will be updated later for the ease of use in the later coming versions of SKIL. For using the Feedback endpoint, you'll have to take care of the following points:\n",
    "1. Add the correct labels in correct order in the `model_labels` argument as mentioned above while adding model through `ModelInstanceEntity`.\n",
    "2. Add a temporary evaluation and associate it with the model. We did this step in the previous paragraph while using `add_evaluation_result` with `EvaluationResultsEntity` object.\n",
    "3. Create a minibatch. A minibatch is a batch that's associated with an evaluation. This is so we know what minibatch was associated while we read the evaluation results.\n",
    "4. Add examples to the minibatch. Examples are the individual inputs inside a minibatch. So, a minibatch of size 10000 would contain 10000 examples. \n",
    "5. Calculate the feedback through the Feedback endpoint.\n",
    "\n",
    "### Note \n",
    "Currently, there's no way to associate metadata to a minibatch and its examples. They're just placeholders to be used with later versions of SKIL.\n",
    "\n",
    "Now, we just need to create the minibatch and its examples before we can use the Feedback endpoint.\n",
    "\n",
    "## Adding a minibatch and its examples for a model's evaluation\n",
    "We'll calculate the evaluation for the test minibatch. While adding the minibatch, make sure that the `eval_id` and `eval_version` matches the evaluation id and evaluation version of the the evaluation you're going to associate this minibatch with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_version': 0,\n",
      " 'eval_id': 'jupyter_model_eval_id_test',\n",
      " 'eval_version': 2,\n",
      " 'mini_batch_id': 'test_minibatch'}\n",
      "{'batch_size': 10000,\n",
      " 'minibatch': {'batch_version': 0,\n",
      "               'eval_id': 'jupyter_model_eval_id_test',\n",
      "               'eval_version': 2,\n",
      "               'mini_batch_id': 'test_minibatch'}}\n"
     ]
    }
   ],
   "source": [
    "minibatch_id = \"test_minibatch\"\n",
    "minibatch_size = len(test_labels)\n",
    "\n",
    "minibatch = MinibatchEntity(\n",
    "    mini_batch_id=minibatch_id,\n",
    "    batch_version=0,\n",
    "    eval_id=eval_id_test, # Evaluation id and evaluation version should match here\n",
    "    eval_version=2\n",
    ")\n",
    "\n",
    "minibatch_response = api_instance.add_minibatch(\n",
    "    model_history_server_id,\n",
    "    minibatch\n",
    ")\n",
    "\n",
    "pp.pprint(minibatch_response)\n",
    "\n",
    "examples_response = api_instance.add_example_for_batch(\n",
    "    model_history_server_id,\n",
    "    AddExampleRequest(\n",
    "        minibatch=minibatch,\n",
    "        batch_size=minibatch_size\n",
    "    )\n",
    ")\n",
    "\n",
    "pp.pprint(examples_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing the feedback endpoint to calculate and save a model's evalution\n",
    "Now we can use the advantages of the feedback endpoint and get all of the evaluation parameters calculated automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback calculated successfully!\n"
     ]
    }
   ],
   "source": [
    "feedback = api_instance.add_model_feedback(\n",
    "    model_history_server_id,\n",
    "    ModelFeedBackRequest(\n",
    "        batch_id=minibatch_id,\n",
    "        guesses=test_guesses.tolist(),\n",
    "        correct=test_correct.tolist()\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Feedback calculated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a deployment\n",
    "Let's deploy our model and start serving it in a deployment. First, we'll have to create a SKIL deployment to contain our model that's going to be served."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'body': {'knn': [], 'models': [], 'transforms': []},\n",
      " 'deployment_slug': 'deployment_jupyter',\n",
      " 'id': '0',\n",
      " 'name': 'deployment_jupyter',\n",
      " 'status': 'Not Deployed'}\n"
     ]
    }
   ],
   "source": [
    "deployment_name = \"deployment_jupyter\"\n",
    "create_deployment_request = CreateDeploymentRequest(deployment_name)\n",
    "deployment_response = api_instance.deployment_create(create_deployment_request)\n",
    "\n",
    "pp.pprint(deployment_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying a Model\n",
    "Let's import the model we trained to the created deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created': 1536507230664,\n",
      " 'deployment_id': 0,\n",
      " 'extra_args': None,\n",
      " 'file_location': None,\n",
      " 'id': 0,\n",
      " 'jvm_args': None,\n",
      " 'labels_file_location': None,\n",
      " 'launch_policy': {'@class': 'io.skymind.deployment.launchpolicy.DefaultLaunchPolicy',\n",
      "                   'maxFailuresMs': 300000,\n",
      "                   'maxFailuresQty': 3},\n",
      " 'model_state': None,\n",
      " 'model_type': 'model',\n",
      " 'name': 'tf_model',\n",
      " 'scale': 1.0,\n",
      " 'state': 'stopped',\n",
      " 'sub_type': None,\n",
      " 'updated': None}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tf_model\"\n",
    "uris = [\"{}/model/{}/default\".format(deployment_name, model_name),\n",
    "        \"{}/model/{}/v1\".format(deployment_name, model_name)]\n",
    "\n",
    "deploy_model_request = ImportModelRequest(model_name,\n",
    "                                          1, \n",
    "                                          file_location=model_file_path,\n",
    "                                          model_type=\"model\",\n",
    "                                          uri=uris,\n",
    "                                          input_names=[\"input_node\", \"keep_prob_input\"], \n",
    "                                          output_names=[\"output_node\"])\n",
    "\n",
    "model_deployment_response = api_instance.deploy_model(deployment_response.id, deploy_model_request)\n",
    "pp.pprint(model_deployment_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a Model to Serve\n",
    "To server a model, you'll need to start a model server that'll serve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created': 1536507230664,\n",
      " 'deployment_id': 0,\n",
      " 'extra_args': None,\n",
      " 'file_location': None,\n",
      " 'id': 0,\n",
      " 'jvm_args': None,\n",
      " 'labels_file_location': None,\n",
      " 'launch_policy': {'@class': 'io.skymind.deployment.launchpolicy.DefaultLaunchPolicy',\n",
      "                   'maxFailuresMs': 300000,\n",
      "                   'maxFailuresQty': 3},\n",
      " 'model_state': None,\n",
      " 'model_type': 'model',\n",
      " 'name': 'tf_model',\n",
      " 'scale': 1.0,\n",
      " 'state': 'starting',\n",
      " 'sub_type': None,\n",
      " 'updated': 1536507237241}\n",
      "\n",
      "Start serving model...\n",
      "wait...\n",
      "Model server started successfully!\n"
     ]
    }
   ],
   "source": [
    "model_state_change_response = api_instance.model_state_change(deployment_response.id,\n",
    "                                                              model_deployment_response.id,\n",
    "                                                              SetState(\"start\"))\n",
    "pp.pprint(model_state_change_response)\n",
    "\n",
    "import time\n",
    "\n",
    "# Checking if the model is already started\n",
    "print(\"\\nStart serving model...\")\n",
    "while True:\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Query the model state\n",
    "    model_state = api_instance.model_state_change(deployment_response.id, \n",
    "                                    model_deployment_response.id, \n",
    "                                    SetState(\"start\")).state\n",
    "    \n",
    "    if model_state == \"started\":\n",
    "      time.sleep(5)\n",
    "      print(\"Model server started successfully!\")\n",
    "      break\n",
    "    else:\n",
    "      print(\"wait...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying an image through the Served Endpoint\n",
    "Now, after the model has been started, we can use the Prediction endpoints to classify our external examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '12345',\n",
      " 'needs_pre_processing': False,\n",
      " 'outputs': [{'array': 'AAdKQVZBQ1BQAAAACAADSU5UAAAAAgAAAAEAAAAKAAAAAQAAAAEAAAAAAAAAAQAAAGMAB0pBVkFD\\r\\n'\n",
      "                       'UFAAAAAKAAVGTE9BVD9/9Pw0bkErONU3nTU1LxAzn2tROAKCsjfcUgM1DIscNvO/MjSWGJc=\\r\\n',\n",
      "              'data': [0.9998319,\n",
      "                       2.2189185e-07,\n",
      "                       0.0001016699,\n",
      "                       6.749624e-07,\n",
      "                       7.423535e-08,\n",
      "                       3.1116135e-05,\n",
      "                       2.6264233e-05,\n",
      "                       5.2356495e-07,\n",
      "                       7.2642224e-06,\n",
      "                       2.795757e-07],\n",
      "              'data_type': None,\n",
      "              'ordering': 'c',\n",
      "              'shape': [1, 10]}]}\n",
      "Actual Label: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "example_index = 10\n",
    "input = test_images[example_index]\n",
    "label = test_labels[example_index]\n",
    "\n",
    "classification_response = api_instance.multipredict(\n",
    "    deployment_name=deployment_name,\n",
    "    model_name=model_name,\n",
    "    version_name=\"default\",\n",
    "    body=MultiPredictRequest(\n",
    "        id=\"12345\",\n",
    "        needs_pre_processing=False,\n",
    "        inputs=[\n",
    "            # This is the actual image data\n",
    "            INDArray(\n",
    "                ordering='c',\n",
    "                shape=[int(len(input))],\n",
    "                data=input.tolist()\n",
    "            ),\n",
    "            # This is the keep_prob placeholder data\n",
    "            INDArray(\n",
    "                ordering='c',\n",
    "                shape=[1],\n",
    "                data=[1.0]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "pp.pprint(classification_response)\n",
    "print('Actual Label: {}'.format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
